# LocalRAGent ğŸ¤–

RAG (Retrieval-Augmented Generation) system built with LangChain, LangGraph, and Llama 3.1, featuring hybrid search, document grading, and routing capabilities.


## ğŸ“ Directory Structure

```
LocalRAGent/
â”œâ”€â”€ api/                        # FastAPI REST API
â”‚   â”œâ”€â”€ main.py                # API entry point
â”‚   â”œâ”€â”€ models/                # Pydantic models
â”‚   â”‚   â””â”€â”€ schemas.py         # Request/response schemas
â”‚   â””â”€â”€ routers/               # API route handlers
â”‚       â”œâ”€â”€ chat.py            # Chat endpoints
â”‚       â””â”€â”€ health.py          # Health check endpoints
â”œâ”€â”€ config/                     # Configuration files
â”‚   â”œâ”€â”€ settings.py            # Application settings
â”‚   â””â”€â”€ prompts.py             # LLM prompts
â”œâ”€â”€ data/                       # Data storage
â”‚   â”œâ”€â”€ processed/             # Processed documents
â”‚   â”œâ”€â”€ raw/                   # Raw input data
â”‚   â”œâ”€â”€ static/                # Static assets
â”‚   â””â”€â”€ vectorstore/           # ChromaDB vector storage
â”œâ”€â”€ src/                        # Core application code
â”‚   â”œâ”€â”€ agents/                # Intelligent agents
â”‚   â”‚   â”œâ”€â”€ rag_agent.py       # RAG generation agent
â”‚   â”‚   â”œâ”€â”€ router_agent.py    # Query routing agent
â”‚   â”‚   â””â”€â”€ web_search_agent.py # Web search agent
â”‚   â”œâ”€â”€ core/                  # Core functionality
â”‚   â”‚   â”œâ”€â”€ document_processor.py # Document processing
â”‚   â”‚   â”œâ”€â”€ embeddings.py      # Embedding management
â”‚   â”‚   â”œâ”€â”€ llm_client.py      # LLM client wrapper
â”‚   â”‚   â””â”€â”€ retriever.py       # Hybrid retrieval system
â”‚   â”œâ”€â”€ graders/               # Quality assessment
â”‚   â”‚   â”œâ”€â”€ answer_grader.py   # Answer quality grading
â”‚   â”‚   â”œâ”€â”€ hallucination_grader.py # Hallucination detection
â”‚   â”‚   â””â”€â”€ relevance_grader.py # Document relevance grading
â”‚   â”œâ”€â”€ utils/                 # Utility functions
â”‚   â”‚   â”œâ”€â”€ document_utils.py  # Document helpers
â”‚   â”‚   â”œâ”€â”€ exceptions.py      # Custom exceptions
â”‚   â”‚   â””â”€â”€ logging_config.py  # Logging configuration
â”‚   â””â”€â”€ workflow/              # LangGraph workflow
â”‚       â”œâ”€â”€ edges.py           # Workflow edge logic
â”‚       â”œâ”€â”€ graph_state.py     # State management
â”‚       â”œâ”€â”€ nodes.py           # Workflow nodes
â”‚       â””â”€â”€ workflow_builder.py # Workflow construction
â”œâ”€â”€ scripts/                    # Utility scripts
â”‚   â””â”€â”€ setup_vectorstore.py  # Vector store initialization
â”œâ”€â”€ tests/                      # Test suite
â”œâ”€â”€ logs/                       # Application logs
â”œâ”€â”€ main.py                     # CLI entry point
â””â”€â”€ requirements.txt           # Python dependencies
```

### Prerequisites
- Python 3.8+
- Ollama (for local Llama 3.1 model)
- API keys for FireCrawl and Tavily

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/SohamD34/LocalRAGent.git
cd LocalRAGent
```

2. **Install dependencies**
```bash
pip install -r requirements.txt
```

3. **Set up environment variables**
Create a `.env` file in the root directory:
```env
LANGCHAIN_API_KEY=your_langchain_api_key
FIRECRAWL_API_KEY=your_firecrawl_api_key
TAVILY_API_KEY=your_tavily_api_key
```

4. **Install and start Ollama**
```bash
# Install Ollama (macOS/Linux)
curl -fsSL https://ollama.ai/install.sh | sh

# Pull Llama 3.1 model
ollama pull llama3.1:8b
```

5. **Initialize the vector store**
```bash
python scripts/setup_vectorstore.py
```

### Usage Options

#### 1. **Command Line Interface**
```bash
python main.py
```

#### 2. **REST API Server**
```bash
# Start the API server
uvicorn api.main:app --reload --host 0.0.0.0 --port 8000

# Test with curl
curl -X POST "http://localhost:8000/chat" \
  -H "Content-Type: application/json" \
  -d '{"question": "What is prompt engineering?"}'
```

#### 3. **Python Integration**
```python
from config.settings import Settings
from src.workflow.workflow_builder import RAGWorkflowBuilder

# Initialize the system
settings = Settings()
builder = RAGWorkflowBuilder(settings)
app = builder.build_workflow()

# Query the system
inputs = {"question": "How to reduce LLM costs?"}
for output in app.stream(inputs):
    print(output)
```
